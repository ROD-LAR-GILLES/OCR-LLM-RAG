# interfaces/cli/llm_menu.py
"""
Interfaz CLI espec√≠fica para procesamiento LLM con pymupdf4llm.

Este m√≥dulo proporciona una interfaz de l√≠nea de comandos simplificada
y optimizada espec√≠ficamente para usar pymupdf4llm en aplicaciones de
modelos de lenguaje, sistemas RAG y an√°lisis por IA.

Diferencias con menu.py tradicional:
- Enfoque espec√≠fico en casos de uso LLM
- Configuraciones predefinidas para RAG, embeddings y an√°lisis
- Feedback detallado sobre chunks y optimizaci√≥n
- Recomendaciones espec√≠ficas para uso con LLMs
"""
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

try:
    import questionary
except ImportError:
    raise ImportError("questionary es requerido. Inst√°lalo con: pip install questionary")

from utils.file_utils import discover_pdf_files, validate_pdf_exists
from application.llm_controllers import LLMDocumentController, LLMProcessingConfig


# Configuraci√≥n de directorios
PDF_DIR = Path("/app/pdfs")        # Directorio de entrada
OUT_DIR = Path("/app/resultado")   # Directorio de salida


def display_llm_welcome() -> None:
    """Muestra mensaje de bienvenida espec√≠fico para LLM."""
    print("\n" + "="*60)
    print("PDF-to-LLM - Procesador optimizado para modelos de lenguaje")
    print("Convierte PDFs a formato markdown estructurado usando pymupdf4llm")
    print("="*60)
    print("\nCasos de uso soportados:")
    print("   ‚Ä¢ RAG Systems (Retrieval-Augmented Generation)")
    print("   ‚Ä¢ Vector Databases y Embeddings")
    print("   ‚Ä¢ An√°lisis automatizado por LLMs")
    print("   ‚Ä¢ B√∫squeda sem√°ntica e indexaci√≥n")
    print("\nTecnolog√≠a: pymupdf4llm + arquitectura hexagonal")
    print("-"*60)


def select_use_case() -> str:
    """
    Permite al usuario seleccionar el caso de uso objetivo.
    
    Returns:
        str: Caso de uso seleccionado ('rag', 'embeddings', 'analysis')
    """
    print("\nüéØ Selecciona el caso de uso objetivo:")
    
    use_case = questionary.select(
        "¬øPara qu√© vas a usar el contenido procesado?",
        choices=[
            questionary.Choice(
                title="Sistema RAG (Retrieval-Augmented Generation)",
                value="rag"
            ),
            questionary.Choice(
                title="üßÆ Vector Database / Embeddings",
                value="embeddings"
            ),
            questionary.Choice(
                title="üî¨ An√°lisis por LLMs",
                value="analysis"
            ),
            questionary.Choice(
                title="‚öôÔ∏è Configuraci√≥n personalizada",
                value="custom"
            ),
            questionary.Choice(
                title="üîô Volver al men√∫ principal",
                value="exit"
            )
        ],
        instruction="Usa ‚Üë‚Üì para navegar, Enter para seleccionar"
    ).ask()
    
    return use_case or "exit"


def configure_custom_settings() -> LLMProcessingConfig:
    """
    Permite configuraci√≥n personalizada de par√°metros.
    
    Returns:
        LLMProcessingConfig: Configuraci√≥n personalizada
    """
    print("\n‚öôÔ∏è Configuraci√≥n personalizada:")
    
    # Tama√±o de chunk
    chunk_size = questionary.text(
        "Tama√±o m√°ximo de chunk (caracteres):",
        default="1000",
        validate=lambda x: x.isdigit() and 100 <= int(x) <= 10000
    ).ask()
    
    # Estrategia de chunking
    semantic_chunking = questionary.confirm(
        "¬øUsar chunking sem√°ntico (recomendado)?",
        default=True
    ).ask()
    
    # Preservar estructura
    preserve_structure = questionary.confirm(
        "¬øPreservar estructura de encabezados en chunks?",
        default=True
    ).ask()
    
    # Overlap
    overlap_percent = questionary.select(
        "Solapamiento entre chunks:",
        choices=[
            questionary.Choice("10% (m√≠nimo)", "0.10"),
            questionary.Choice("20% (recomendado)", "0.20"),
            questionary.Choice("30% (m√°ximo)", "0.30")
        ]
    ).ask()
    
    # Configuraciones adicionales
    include_images = questionary.confirm(
        "¬øIncluir informaci√≥n sobre im√°genes?",
        default=False
    ).ask()
    
    save_chunks = questionary.confirm(
        "¬øGuardar chunks en archivos separados?",
        default=True
    ).ask()
    
    export_json = questionary.confirm(
        "¬øExportar datos completos en JSON?",
        default=True
    ).ask()
    
    return LLMProcessingConfig(
        chunk_size=int(chunk_size),
        chunk_overlap=int(int(chunk_size) * float(overlap_percent)),
        semantic_chunking=semantic_chunking,
        preserve_structure=preserve_structure,
        extract_images=include_images,
        save_chunks_separately=save_chunks,
        export_json=export_json,
        optimization_target="custom"
    )


def select_pdf_file() -> Optional[Path]:
    """
    Permite seleccionar un archivo PDF del directorio de entrada.
    
    Returns:
        Optional[Path]: Ruta al PDF seleccionado o None si se cancela
    """
    print(f"\nBuscando archivos PDF en: {PDF_DIR}")
    
    try:
        pdf_files = discover_pdf_files(PDF_DIR)
        
        if not pdf_files:
            print("[ERROR] No se encontraron archivos PDF en el directorio.")
            print(f"   Coloca tus archivos PDF en: {PDF_DIR}")
            return None
            
        print(f"[OK] Encontrados {len(pdf_files)} archivos PDF")
        
        # Crear opciones para el men√∫
        choices = []
        for pdf_file in pdf_files:
            file_size = pdf_file.stat().st_size / (1024 * 1024)  # MB
            title = f"üìÑ {pdf_file.name} ({file_size:.1f} MB)"
            choices.append(questionary.Choice(title, pdf_file))
        
        choices.append(questionary.Choice("üîô Volver al men√∫ principal", None))
        
        selected = questionary.select(
            "Selecciona el archivo PDF a procesar:",
            choices=choices,
            instruction="Usa ‚Üë‚Üì para navegar, Enter para seleccionar"
        ).ask()
        
        return selected
        
    except Exception as e:
        print(f"[ERROR] Error buscando archivos PDF: {str(e)}")
        return None


def display_processing_progress(pdf_path: Path, config: LLMProcessingConfig) -> None:
    """
    Muestra informaci√≥n sobre el procesamiento que se va a realizar.
    
    Args:
        pdf_path (Path): Archivo que se va a procesar
        config (LLMProcessingConfig): Configuraci√≥n que se va a usar
    """
    print(f"\nüîÑ Procesando: {pdf_path.name}")
    print(f"üìä Configuraci√≥n:")
    print(f"   ‚Ä¢ Caso de uso: {config.optimization_target}")
    print(f"   ‚Ä¢ Tama√±o de chunk: {config.chunk_size} caracteres")
    print(f"   ‚Ä¢ Solapamiento: {config.chunk_overlap} caracteres")
    print(f"   ‚Ä¢ Chunking sem√°ntico: {'[OK]' if config.semantic_chunking else '[NO]'}")
    print(f"   ‚Ä¢ Preservar estructura: {'[OK]' if config.preserve_structure else '[NO]'}")
    print(f"   ‚Ä¢ Chunks separados: {'[OK]' if config.save_chunks_separately else '[NO]'}")
    print("\n‚è≥ Procesando con pymupdf4llm...")


def display_results(result: Dict[str, Any]) -> None:
    """
    Muestra los resultados del procesamiento de forma detallada.
    
    Args:
        result (Dict[str, Any]): Resultado del procesamiento
    """
    if not result.get("success", False):
        print(f"\n[ERROR] Error en el procesamiento:")
        print(f"   {result.get('error', 'Error desconocido')}")
        
        recommendations = result.get("recommendations", [])
        if recommendations:
            print(f"\nüí° Recomendaciones:")
            for rec in recommendations:
                print(f"   ‚Ä¢ {rec}")
        return
    
    print(f"\n[OK] Procesamiento completado exitosamente!")
    
    # Informaci√≥n b√°sica
    chunk_info = result.get("chunk_info", {})
    processing_stats = result.get("processing_stats", {})
    
    print(f"\nüìä Estad√≠sticas del documento:")
    print(f"   ‚Ä¢ Chunks generados: {chunk_info.get('total_chunks', 0)}")
    print(f"   ‚Ä¢ Tama√±o promedio de chunk: {processing_stats.get('avg_chunk_size', 0):.0f} caracteres")
    print(f"   ‚Ä¢ Tokens estimados totales: {processing_stats.get('total_tokens_estimated', 0):.0f}")
    print(f"   ‚Ä¢ Coherencia sem√°ntica: {processing_stats.get('semantic_coherence', 0):.2f}")
    
    # Distribuci√≥n por secciones
    sections_dist = chunk_info.get("sections_distribution", {})
    if sections_dist:
        print(f"\nüìë Distribuci√≥n por secciones:")
        for section, count in list(sections_dist.items())[:5]:  # Mostrar top 5
            section_name = section if len(section) <= 40 else section[:37] + "..."
            print(f"   ‚Ä¢ {section_name}: {count} chunks")
    
    # Archivos generados
    files_generated = result.get("files_generated", [])
    print(f"\nüìÅ Archivos generados ({len(files_generated)}):")
    for file_path in files_generated[:10]:  # Mostrar primeros 10
        file_name = Path(file_path).name
        print(f"   ‚Ä¢ {file_name}")
    
    if len(files_generated) > 10:
        print(f"   ... y {len(files_generated) - 10} archivos m√°s")
    
    # Recomendaciones espec√≠ficas para LLMs
    recommendations = result.get("recommendations", [])
    if recommendations:
        print(f"\nüí° Recomendaciones para uso con LLMs:")
        for rec in recommendations:
            print(f"   ‚Ä¢ {rec}")
    
    # Informaci√≥n de salida
    output_info = result.get("controller_info", {})
    output_dir = output_info.get("output_directory", "desconocido")
    print(f"\nArchivos guardados en: {output_dir}")


def display_batch_results(batch_result: Dict[str, Any]) -> None:
    """
    Muestra resultados de procesamiento en lote.
    
    Args:
        batch_result (Dict[str, Any]): Resultado del procesamiento en lote
    """
    stats = batch_result.get("aggregate_stats", {})
    
    print(f"\nüìä Resumen del procesamiento en lote:")
    print(f"   ‚Ä¢ Documentos totales: {batch_result.get('total_documents', 0)}")
    print(f"   ‚Ä¢ Exitosos: {batch_result.get('successful', 0)}")
    print(f"   ‚Ä¢ Fallidos: {batch_result.get('failed', 0)}")
    print(f"   ‚Ä¢ Tasa de √©xito: {stats.get('success_rate', 0):.1%}")
    print(f"   ‚Ä¢ Chunks totales: {stats.get('total_chunks_generated', 0)}")
    print(f"   ‚Ä¢ Archivos generados: {stats.get('total_files_generated', 0)}")


def process_single_document() -> None:
    """Flujo para procesar un solo documento."""
    # Seleccionar caso de uso
    use_case = select_use_case()
    if use_case == "exit":
        return
    
    # Seleccionar archivo
    pdf_path = select_pdf_file()
    if pdf_path is None:
        return
    
    # Configurar procesamiento
    controller = LLMDocumentController(OUT_DIR)
    
    if use_case == "custom":
        config = configure_custom_settings()
    else:
        config = controller.get_recommended_config(use_case)
    
    # Mostrar configuraci√≥n
    display_processing_progress(pdf_path, config)
    
    # Procesar documento
    try:
        result = controller.process_document(pdf_path, config)
        display_results(result)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Procesamiento cancelado por el usuario")
    except Exception as e:
        print(f"\n[ERROR] Error inesperado: {str(e)}")


def process_batch_documents() -> None:
    """Flujo para procesar m√∫ltiples documentos en lote."""
    print(f"\nüìÅ Buscando archivos PDF en: {PDF_DIR}")
    
    try:
        pdf_files = discover_pdf_files(PDF_DIR)
        
        if not pdf_files:
            print("[ERROR] No se encontraron archivos PDF en el directorio.")
            return
            
        print(f"[OK] Encontrados {len(pdf_files)} archivos PDF")
        
        # Confirmar procesamiento en lote
        confirm = questionary.confirm(
            f"¬øProcesar todos los {len(pdf_files)} archivos PDF?",
            default=False
        ).ask()
        
        if not confirm:
            return
        
        # Seleccionar configuraci√≥n
        use_case = select_use_case()
        if use_case == "exit":
            return
        
        controller = LLMDocumentController(OUT_DIR)
        
        if use_case == "custom":
            config = configure_custom_settings()
        else:
            config = controller.get_recommended_config(use_case)
        
        print(f"\nüîÑ Procesando {len(pdf_files)} documentos...")
        print("‚è≥ Esto puede tomar varios minutos...")
        
        # Procesar en lote
        batch_result = controller.process_multiple_documents(pdf_files, config)
        display_batch_results(batch_result)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Procesamiento en lote cancelado por el usuario")
    except Exception as e:
        print(f"\n[ERROR] Error en procesamiento en lote: {str(e)}")


def show_statistics() -> None:
    """Muestra estad√≠sticas del directorio de salida."""
    controller = LLMDocumentController(OUT_DIR)
    stats = controller.get_processing_statistics()
    
    if "error" in stats:
        print(f"\n[ERROR] Error obteniendo estad√≠sticas: {stats['error']}")
        return
    
    print(f"\nüìä Estad√≠sticas del directorio de salida:")
    print(f"   ‚Ä¢ Archivos totales: {stats.get('total_files', 0)}")
    print(f"   ‚Ä¢ Tama√±o total: {stats.get('total_size_mb', 0)} MB")
    print(f"   ‚Ä¢ Directorio: {stats.get('output_directory', 'desconocido')}")
    
    recent_files = stats.get("recent_files", [])
    if recent_files:
        print(f"\nüìù Archivos recientes:")
        for file_name in recent_files:
            print(f"   ‚Ä¢ {file_name}")


def main_llm_menu() -> None:
    """Men√∫ principal para procesamiento LLM."""
    display_llm_welcome()
    
    while True:
        try:
            choice = questionary.select(
                "\n¬øQu√© deseas hacer?",
                choices=[
                    questionary.Choice(
                        "üìÑ Procesar un documento PDF",
                        "single"
                    ),
                    questionary.Choice(
                        "Procesar m√∫ltiples documentos (lote)",
                        "batch"
                    ),
                    questionary.Choice(
                        "üìä Ver estad√≠sticas de procesamiento",
                        "stats"
                    ),
                    questionary.Choice(
                        "‚ÑπÔ∏è Informaci√≥n sobre casos de uso",
                        "info"
                    ),
                    questionary.Choice(
                        "üö™ Salir",
                        "exit"
                    )
                ],
                instruction="Usa ‚Üë‚Üì para navegar, Enter para seleccionar"
            ).ask()
            
            if choice == "single":
                process_single_document()
            elif choice == "batch":
                process_batch_documents()
            elif choice == "stats":
                show_statistics()
            elif choice == "info":
                show_use_case_info()
            elif choice == "exit":
                print("\nüëã ¬°Hasta luego!")
                break
            else:
                print("\n‚ö†Ô∏è Opci√≥n no v√°lida")
                
        except KeyboardInterrupt:
            print("\n\nüëã ¬°Hasta luego!")
            break
        except Exception as e:
            print(f"\n[ERROR] Error en el men√∫: {str(e)}")


def show_use_case_info() -> None:
    """Muestra informaci√≥n detallada sobre los casos de uso."""
    print("\nüìö Informaci√≥n sobre casos de uso:")
    print("-" * 50)
    
    print("\nüîç Sistema RAG (Retrieval-Augmented Generation):")
    print("   ‚Ä¢ Chunks de ~1000 caracteres con 20% de solapamiento")
    print("   ‚Ä¢ Preserva estructura de encabezados para contexto")
    print("   ‚Ä¢ Metadatos enriquecidos para mejor retrieval")
    print("   ‚Ä¢ Ideal para: ChatGPT, Claude, sistemas de Q&A")
    
    print("\nüßÆ Vector Database / Embeddings:")
    print("   ‚Ä¢ Chunks m√°s peque√±os (~512 caracteres)")
    print("   ‚Ä¢ Menor solapamiento para evitar redundancia")
    print("   ‚Ä¢ Optimizado para modelos de embeddings")
    print("   ‚Ä¢ Ideal para: b√∫squeda sem√°ntica, clustering")
    
    print("\nüî¨ An√°lisis por LLMs:")
    print("   ‚Ä¢ Chunks m√°s grandes (~2000 caracteres)")
    print("   ‚Ä¢ Incluye im√°genes y elementos visuales")
    print("   ‚Ä¢ Genera res√∫menes autom√°ticos")
    print("   ‚Ä¢ Ideal para: an√°lisis de contenido, extracci√≥n de insights")
    
    print("\nüí° Tecnolog√≠a utilizada:")
    print("   ‚Ä¢ pymupdf4llm: Extracci√≥n optimizada para LLMs")
    print("   ‚Ä¢ Markdown estructurado con preservaci√≥n de jerarqu√≠a")
    print("   ‚Ä¢ Chunking sem√°nticamente coherente")
    print("   ‚Ä¢ Metadatos enriquecidos para contexto")


if __name__ == "__main__":
    main_llm_menu()
